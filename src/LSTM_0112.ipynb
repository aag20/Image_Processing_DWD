{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45fbf6f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from datetime import datetime, timedelta\n",
    "from keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, Bidirectional, BatchNormalization\n",
    "from keras.layers import Conv2D, MaxPooling2D,UpSampling3D, Conv3DTranspose, Flatten, Concatenate, Dense, TimeDistributed, Bidirectional, Input, Reshape  \n",
    "from keras.models import Model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.layers import LSTM\n",
    "from keras.callbacks import EarlyStopping\n",
    "from time import sleep\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccb18aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "parquet_df =  pd.read_parquet(\"/home/arman_abouali/Downloads/DWD/Original_files/DWD_window/X_data_window.parquet\")\n",
    "parquet_df = parquet_df.sort_values(by='Key', ascending=True)\n",
    "parquet_df['Key'] = pd.to_datetime(parquet_df['Key'], format='%Y%m%d_%H%M')\n",
    "#parquet_df['Image_Sum'] = parquet_df['Value'].apply(lambda x: sum(sum(row) for row in x))\n",
    "parquet_df.reset_index(drop=True, inplace=True)\n",
    "#parquet_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "743545a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now you can perform your operations\n",
    "parquet_idx = pd.date_range(\"2003-11-01 00:00:00\", \"2017-12-31 23:45:00\", freq=\"15min\")\n",
    "parquet_df.reset_index(drop=True, inplace=True)\n",
    "parquet_df = parquet_df.set_index(parquet_idx)\n",
    "parquet_df = parquet_df.reindex(parquet_idx)\n",
    "parquet_df = parquet_df.drop('Key', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8a3d87a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_list_of_arrays_and_divide(array_list):\n",
    "    # Concatenate the arrays in the list\n",
    "    concatenated_array = np.concatenate(array_list)\n",
    "    # Divide every element by 10\n",
    "    divided_array = concatenated_array / 10\n",
    "    # Flatten the array\n",
    "    return divided_array.flatten()\n",
    "\n",
    "# Apply this updated function to each element in the 'Value' column\n",
    "parquet_df['Value'] = parquet_df['Value'].apply(flatten_list_of_arrays_and_divide)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46819227",
   "metadata": {},
   "outputs": [],
   "source": [
    "parquet_df['Image_Sum'] = parquet_df['Value'].apply(np.sum)\n",
    "\n",
    "parquet_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "588b5c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_df = pd.read_csv('/home/arman_abouali/Downloads/DWD/input.csv', sep=';')\n",
    "\n",
    "input_df['Zeit'] = input_df['Zeit'].replace(\"24:00:00\", \"00:00:00\")\n",
    "input_df['Zeit'] = pd.to_datetime(input_df['Datum'] + ' ' + input_df['Zeit'], format='%d.%m.%Y %H:%M:%S')\n",
    "input_df = input_df.drop('Datum', axis=1)\n",
    "input_df = input_df.sort_values(by='Zeit', ascending=True).reset_index(drop=True)\n",
    "input_df['Sensor_Sum'] = input_df['GranetalsperreMin15Niederschlag'] + input_df['HahnenkleeMin15Niederschlag'] + input_df['Niederschlag_Gosequelle'] + input_df['Niederschlag_Abzuchtquelle']\n",
    "input_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad34cca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = pd.date_range(\"2003-11-01 00:00:00\", \"2018-06-30 23:45:00\", freq=\"15min\")\n",
    "input_df.reset_index(drop=True, inplace=True)\n",
    "input_df = input_df.set_index(idx)\n",
    "input_df = input_df.reindex(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12605ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_df = input_df.drop('Zeit', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f31bb2f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the DataFrames on their indices\n",
    "merged_df = pd.merge(input_df, parquet_df, left_index=True, right_index=True, how='inner')\n",
    "merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c8be3f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.to_csv('merged_df.csv', index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c295d46b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_images = np.array(merged_df['Value'].tolist())\n",
    "\n",
    "y = np.array(merged_df['SennhuetteMin15W'].tolist())\n",
    "y = y.reshape(-1, 1)\n",
    "\n",
    "print(f\"Shape of X: {X_images.shape}\")\n",
    "print(f\"Shape of y: {y.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "066918b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Reshape y to be a 2D array with one column\n",
    "y = y.reshape(-1, 1)\n",
    "y = scaler.fit_transform(y)\n",
    "\n",
    "print(f\"Shape of y_scaled: {y.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94c2e410",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "X_images_scaled = scaler.fit_transform(X_images.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e347b9f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sequences(array, sequence_length):\n",
    "    X = []\n",
    "    for i in range(len(array)-sequence_length):\n",
    "        end_idx = i + sequence_length\n",
    "        sequence_x = array[i:end_idx]\n",
    "        X.append(sequence_x)\n",
    "\n",
    "    return np.array(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bc77777",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the sequence length\n",
    "seq_length = 96\n",
    "X_images_sequence = create_sequences(X_images, sequence_length=seq_length)\n",
    "\n",
    "# The labels for each sequence are the water level values aligned with the end of each sequence\n",
    "y_sequence = y[seq_length:]\n",
    "\n",
    "# Print out the shapes of the resulting arrays to confirm they're what we expect\n",
    "print(f\"Shape of X sequence: {X_images_sequence.shape}\")\n",
    "print(f\"Shape of y sequence: {y_sequence.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd17fa94",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_len = int(X_images_sequence.shape[0]*0.7)\n",
    "val_len = int(X_images_sequence.shape[0]*0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "091970c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and temporary sets (the latter will be split into validation and test sets)\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(X_images_sequence, y_sequence, test_size=0.1, random_state=42, shuffle=False)\n",
    "\n",
    "# Split the temporary set into validation and training sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.2, random_state=42, shuffle=False)\n",
    "\n",
    "# Print out the shapes of the resulting datasets\n",
    "print(f\"Shape of X_train: {X_train.shape}\")\n",
    "print(f\"Shape of X_val: {X_val.shape}\")\n",
    "print(f\"Shape of X_test: {X_test.shape}\")\n",
    "print(f\"Shape of y_train: {y_train.shape}\")\n",
    "print(f\"Shape of y_val: {y_val.shape}\")\n",
    "print(f\"Shape of y_test: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff9fed90",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "# First LSTM layer with Dropout\n",
    "model.add(LSTM(32, return_sequences=True, input_shape=(X_train.shape[1], X_train.shape[2])))\n",
    "#model.add(Dropout(0.2))\n",
    "\n",
    "# Second LSTM layer\n",
    "model.add(LSTM(32, return_sequences=True , activation='swish'))\n",
    "#model.add(Dropout(0.2))\n",
    "\n",
    "# Third LSTM layer\n",
    "model.add(LSTM(32))\n",
    "#model.add(Dropout(0.2))\n",
    "\n",
    "# Add a Batch Normalization layer\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "# Dense layer\n",
    "model.add(Dense(32, activation='swish'))\n",
    "#model.add(Dropout(0.2))\n",
    "\n",
    "# Output layer\n",
    "model.add(Dense(1, activation='swish'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='mae', optimizer=tf.keras.optimizers.Adam(1e-3), metrics=['mae'])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "callbacks = [\n",
    "    tf.keras.callbacks.EarlyStopping(patience=10, monitor='val_loss'),\n",
    "    tf.keras.callbacks.ModelCheckpoint(filepath='best_model.h5', monitor='val_loss', save_best_only=True)\n",
    "]\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, epochs=100, batch_size=1024, validation_data=(X_val, y_val), callbacks=callbacks)\n",
    "# Save the model\n",
    "model_save_path = '/home/arman_abouali/Downloads/DWD/Original_files/DWD_window/LSTM_model.pb'  # Replace with your own path\n",
    "model.save(model_save_path)\n",
    "\n",
    "print(f\"Model saved to {model_save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2193f433",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.yscale('log')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c09ef46e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction\n",
    "y_hat_train = model.predict(X_train)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(7, 6))  # Create only one plot\n",
    "\n",
    "# Plot the predicted and reference data for the first column\n",
    "ax.plot(y_hat_train[:, 0], label='Predicted 0')\n",
    "ax.plot(y_train[:, 0], label='Reference 0', alpha=.5)\n",
    "ax.legend()\n",
    "ax.set_title('Sennhuette_LSTM Model Prediction')\n",
    "\n",
    "plt.show()  # Display the plot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "001d8b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction\n",
    "y_hat_val = model.predict(X_val)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(7, 6))  # Create only one plot\n",
    "\n",
    "# Plot the predicted and reference data for the first column\n",
    "ax.plot(y_hat_val[:, 0], label='Predicted')\n",
    "ax.plot(y_val[:, 0], label='Reference', alpha=.5)\n",
    "ax.legend()\n",
    "ax.set_title('Sennhuette_LSTM Model Prediction')\n",
    "\n",
    "plt.show()  # Display the plot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9205c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction\n",
    "y_hat_test = model.predict(X_test)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(7, 6))  # Create only one plot\n",
    "\n",
    "# Plot the predicted and reference data for the first column\n",
    "ax.plot(y_hat_test[:, 0], label='Predicted 0')\n",
    "ax.plot(y_test[:, 0], label='Reference 0', alpha=.5)\n",
    "ax.legend()\n",
    "ax.set_title('Sennhuette_LSTM Model Prediction')\n",
    "\n",
    "plt.show()  # Display the plot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0701faa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_1d = np.ravel(y_test)\n",
    "y_hat_test_1d = np.ravel(y_hat_test)\n",
    "\n",
    "# Create the DataFrame\n",
    "pred_df = pd.DataFrame({\n",
    "    'measured': y_test_1d,\n",
    "    'predicted': y_hat_test_1d\n",
    "})\n",
    "\n",
    "# Reset the index so that it starts from 0 and add a new unnamed index column\n",
    "pred_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Save the DataFrame to a CSV file with the unnamed index column\n",
    "pred_df.to_csv('Prediction.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f09b568a",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_df =  pd.read_csv(\"/home/arman_abouali/Downloads/Prediction.csv\")\n",
    "predict_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcb9fab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the metrics\n",
    "def mse(y_true, y_pred):\n",
    "    return ((y_true - y_pred) ** 2).mean()\n",
    "\n",
    "def rmse(y_true, y_pred):\n",
    "    return np.sqrt(mse(y_true, y_pred))\n",
    "\n",
    "def mae(y_true, y_pred):\n",
    "    return np.abs(y_true - y_pred).mean()\n",
    "\n",
    "def r_squared(y_true, y_pred):\n",
    "    ss_res = np.sum((y_true - y_pred) ** 2)\n",
    "    ss_tot = np.sum((y_true - np.mean(y_true)) ** 2)\n",
    "    return 1 - (ss_res / ss_tot)\n",
    "\n",
    "datasets = {\n",
    "    'train': (y_train, y_hat_train),\n",
    "    'test': (y_test, y_hat_test),\n",
    "    'val': (y_val, y_hat_val)\n",
    "}\n",
    "def round_metric(metric_value, decimals=6):\n",
    "    return round(metric_value, decimals)\n",
    "\n",
    "for name, (y_true, y_pred) in datasets.items():\n",
    "    print(f\"Metrics for {name} dataset:\")\n",
    "    print(f\"MSE: {round_metric(mse(y_true, y_pred))}\")\n",
    "    print(f\"RMSE: {round_metric(rmse(y_true, y_pred))}\")\n",
    "    print(f\"MAE: {round_metric(mae(y_true, y_pred))}\")\n",
    "    print(f\"R-squared: {round_metric(r_squared(y_true, y_pred))}\") \n",
    "    print(\"-\" * 30)\n",
    "\n",
    "\n",
    "# List of column names\n",
    "column_names = ['Margarethenklippe_Pegel_now','Sennhuette_Pegel_now']\n",
    "\n",
    "# Function to plot residuals\n",
    "def plot_residuals(y_true, y_pred, column_name):\n",
    "    residuals = y_true - y_pred\n",
    "    plt.scatter(y_pred, residuals, alpha=0.5)\n",
    "    plt.axhline(0, color='r', linestyle='--')\n",
    "    plt.title(f\"Residual Plot for {column_name}\")\n",
    "    plt.xlabel(f\"Predicted Values for {column_name}\")\n",
    "    plt.ylabel(\"Residuals\")\n",
    "    plt.show()\n",
    "\n",
    "# Calculate residuals for each column\n",
    "for i in range(y_test.shape[1]):\n",
    "    y_true_column = y_test[:, i]\n",
    "    y_pred_column = y_hat_test[:, i]\n",
    "    \n",
    "    plot_residuals(y_true_column, y_pred_column, column_names[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2480b237",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(42)\n",
    "custom_index = 500\n",
    "sequence_length = 8\n",
    "\n",
    "# Calculate start index of y_test in y_df based on your provided splits\n",
    "test_start_index = train_len + val_len\n",
    "\n",
    "# Extract corresponding timestamps for y_test\n",
    "test_time_stamps = input_df.index[test_start_index:test_start_index + len(y_test)]\n",
    "\n",
    "# Now extract specific sequence timestamps for the custom index\n",
    "sequence_time_stamps = test_time_stamps[custom_index:custom_index + sequence_length].strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "# Actual and predicted values for the sequence\n",
    "actual_sequence = y_test[custom_index:custom_index + sequence_length]\n",
    "predicted_sequence = y_hat_test[custom_index:custom_index + sequence_length]\n",
    "\n",
    "# Plotting the selected sequence for one column\n",
    "column_name = 'SennhuetteMin15W'\n",
    "\n",
    "plt.figure(figsize=(10, 4))  # Adjust the figure size for better label readability\n",
    "plt.plot(sequence_time_stamps, actual_sequence, label='Actual', marker='o')\n",
    "plt.plot(sequence_time_stamps, predicted_sequence, label='Predicted', marker='x')\n",
    "plt.title(f\"Actual vs Predicted for {column_name}\")\n",
    "plt.xlabel(\"Time stamp\")\n",
    "plt.ylabel(column_name)\n",
    "plt.xticks(rotation=90)  \n",
    "plt.legend()\n",
    "plt.tight_layout() \n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3741e0c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_ground_truth = y_test\n",
    "y_prediction = y_hat_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb3a9d41",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_result = pd.DataFrame(columns = [\"measured\", \"predicted\"], index = range(y_ground_truth.size))\n",
    "df_result [\"measured\"] = y_ground_truth\n",
    "df_result [\"predicted\"] = y_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05410716",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_result.to_csv(\"Arman_090224.csv\", sep = \";\", index = \"0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de428694",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
