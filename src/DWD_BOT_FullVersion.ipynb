{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ac4bafa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "import tarfile\n",
    "import gzip\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "from bs4 import BeautifulSoup\n",
    "from PIL import Image\n",
    "from datetime import datetime, timedelta\n",
    "import rasterio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2bccdc6",
   "metadata": {},
   "source": [
    "### Convert 1D calculated coordinates to 2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0218e463",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1by1 based on new coordinates: [517915, 517916, 519016, 519017]\n",
    "# 1by1 based on old_coordinates: [517445, 517446, 518546, 517446]\n",
    "# 1by1 based on old_coordinates  = [(470, 444), (471, 444), (471, 445), (470, 445)]\n",
    "# Window coordinates: [514612, 514620, 523420, 523428]\n",
    "#coordinates (8*8)= [(467, 444), (467, 452), (475, 444), (475, 452)]\n",
    "def reshape_1D_to_2D(array_1D):\n",
    "    if len(array_1D) != 992001:\n",
    "        raise ValueError(\"The length of the 1D array must be 992,001.\")\n",
    "    \n",
    "    array_2D = np.reshape(array_1D, (901, 1101))\n",
    "    return array_2D.T\n",
    "\n",
    "# Create a 1D array from 1 to 992001\n",
    "array_1D = np.arange(1, 992002)\n",
    "\n",
    "# Use the function to reshape the 1D array into a 2D array\n",
    "try:\n",
    "    array_2D = reshape_1D_to_2D(array_1D)\n",
    "    print(\"Successfully reshaped to 2D array.\")\n",
    "\n",
    "    # Ask user to input a number from the 1D array\n",
    "    user_input = int(input(\"Enter a number from 1 to 992001: \"))\n",
    "    \n",
    "    if user_input >= 1 and user_input <= 992001:\n",
    "        width = 1101\n",
    "        height = 901\n",
    "\n",
    "        index_1D = user_input - 1  # adjust for 0-based index\n",
    "        \n",
    "        row_idx = index_1D // width\n",
    "        col_idx = index_1D % width\n",
    "        \n",
    "        index_2D = (row_idx, col_idx)\n",
    "        print(f\"The 2D index of {user_input} is {index_2D}.\")\n",
    "    else:\n",
    "        print(\"Number out of range.\")\n",
    "\n",
    "except ValueError as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1489219b",
   "metadata": {},
   "source": [
    "### Use the cell below to proceed ove a single file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6a1caad",
   "metadata": {},
   "outputs": [],
   "source": [
    "directory_path = \"/home/arman_abouali/Downloads/DWD\"\n",
    "\n",
    "year = input('Please inster Year as YYYY (2001-2022): ')\n",
    "month = input('Please insert Month as MM (01 - 12): ')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad4b8dc8",
   "metadata": {},
   "source": [
    "### Complete these steps to store and process the image data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2a1abe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_weather_data(directory_path,year,month):\n",
    "        base_url = 'https://opendata.dwd.de/climate_environment/CDC/grids_germany/5_minutes/radolan/reproc/2017_002/asc/'\n",
    "\n",
    "        # Send a GET request to the base URL and get the HTML content\n",
    "        response = requests.get(base_url)\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "        if os.path.exists(directory_path):\n",
    "            os.chdir(directory_path)\n",
    "        else:\n",
    "            print(f\"Directory '{directory_path}' doesn't exist. Please enter an existing directory.\")\n",
    "            return\n",
    "        \n",
    "        print(\"Please be patient!\")\n",
    "\n",
    "        # Construct the URL for the selected weather data file\n",
    "        file_url = base_url + year + '/' + f\"YW2017.002_{year}{month}_asc.tar\"\n",
    "    \n",
    "        # Download the selected weather data file\n",
    "        download_file(file_url, directory_path)\n",
    "   \n",
    "def download_file(url, directory_path):\n",
    "        \n",
    "        # Send a GET request to the specified URL and get the content\n",
    "        response = requests.get(url)\n",
    "\n",
    "        # Construct the file name from the URL\n",
    "        file_name = url.split('/')[-1]\n",
    "\n",
    "        # Construct the file path from the directory path and file name\n",
    "        file_path = os.path.join(directory_path, file_name)\n",
    "\n",
    "        # Save the downloaded content to the file\n",
    "        with open(file_path, 'wb') as f:\n",
    "            f.write(response.content)\n",
    "\n",
    "        print(f\"Downloaded {file_name} to {file_path}\")\n",
    "        print(\"Step 1: Download and Extraction completed!\")\n",
    "        \n",
    "#download_weather_data(directory_path,year,month)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5df1d650",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_tar(directory_path):\n",
    "\n",
    "    # Loop through all the .tar files in the directory and extract them into new folders\n",
    "    for file_name in os.listdir(directory_path):\n",
    "        if file_name.endswith('.tar'):\n",
    "            file_path = os.path.join(directory_path, file_name)\n",
    "\n",
    "            # Create a new folder with the same name as the .tar file\n",
    "            folder_name = file_name.replace('.tar', '')\n",
    "            folder_path = os.path.join(directory_path, folder_name)\n",
    "            if os.path.exists(folder_path):\n",
    "                print(f'Folder {folder_name} already exists. Extracting {file_name} into {folder_name}/')\n",
    "            else:\n",
    "                # print(f'Creating folder {folder_name} and extracting {file_name} into {folder_name}/')\n",
    "                os.makedirs(folder_path, exist_ok=True)\n",
    "\n",
    "            # Extract the .tar file into the new folder\n",
    "            tar = tarfile.open(file_path)\n",
    "            tar.extractall(path=folder_path)\n",
    "            tar.close()\n",
    "\n",
    "            # Remove the .tar file from the directory_path directory\n",
    "            os.remove(file_path)\n",
    "            # print(f'Removed {file_name} from {directory_path}')\n",
    "    print('Step 2: Extraction and cleanup completed!')\n",
    "#extract_tar(directory_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca223616",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Unzip_gz_files(directory_path):\n",
    "    \n",
    "    for root, dirs, files in os.walk(directory_path):\n",
    "        for file in files:\n",
    "            if file.endswith('.gz'):\n",
    "                gz_path = os.path.join(root, file)\n",
    "                folder_path = os.path.splitext(gz_path)[0]\n",
    "                os.makedirs(folder_path, exist_ok=True)\n",
    "                with gzip.open(gz_path, 'rb') as f_in:\n",
    "                    with open(folder_path + '/' + file[:-3], 'wb') as f_out:\n",
    "                        shutil.copyfileobj(f_in, f_out) \n",
    "                os.remove(gz_path)\n",
    "    print('Step 3: Unzip completed!')\n",
    "#Unzip_gz_files(directory_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11353d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_zip(directory_path):\n",
    "    for dirpath, dirnames, filenames in os.walk(directory_path):\n",
    "        for filename in filenames:\n",
    "            if filename.endswith('.tar'):\n",
    "                tar_path = os.path.join(dirpath, filename)\n",
    "                with tarfile.open(tar_path, 'r') as tar:\n",
    "                    tar.extractall(path=dirpath)\n",
    "                os.remove(tar_path)\n",
    "    print('Step 4: Extraction completed!')\n",
    "#extract_zip(directory_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3510bf48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_array(directory_path, year, month):\n",
    "\n",
    "    for day in range(1, 32):\n",
    "        day_str = str(day).zfill(2)\n",
    "        sub_folder_name = f\"YW2017.002_{year}{month}{day_str}_asc.tar\"\n",
    "\n",
    "        sub_folder_path = next(Path(directory_path).rglob(sub_folder_name), None)\n",
    "\n",
    "        if not sub_folder_path:\n",
    "            break\n",
    "        \n",
    "        agg_folder_name = \"agg_\" + sub_folder_name.replace(\".tar\", \"\")\n",
    "        agg_folder_path = Path(sub_folder_path).parent / agg_folder_name\n",
    "        agg_folder_path.mkdir(parents=True, exist_ok=True)\n",
    "        os.chdir(Path(sub_folder_path).parent)\n",
    "\n",
    "        asc_files = sorted(list(Path(sub_folder_path).rglob('*.asc')))\n",
    "\n",
    "        aggregated_files = []\n",
    "        for i in range(0, len(asc_files), 3):\n",
    "            file1 = np.loadtxt(asc_files[i], skiprows=6)\n",
    "            file2 = np.loadtxt(asc_files[i+1], skiprows=6)\n",
    "            file3 = np.loadtxt(asc_files[i+2], skiprows=6)\n",
    "\n",
    "            aggregated_file = np.sum([np.maximum(file1, 0), np.maximum(file2, 0), np.maximum(file3, 0)], axis=0)\n",
    "\n",
    "            # Save as .npy instead of .asc\n",
    "            output_filename = agg_folder_path / (asc_files[i+2].stem + '.npy')\n",
    "            np.save(output_filename, aggregated_file)\n",
    "\n",
    "            # Remove the original .asc files\n",
    "            for f in [asc_files[i], asc_files[i+1], asc_files[i+2]]:\n",
    "                os.remove(f)\n",
    "\n",
    "            aggregated_files.append(aggregated_file)\n",
    "\n",
    "        aggregated_array = np.stack(aggregated_files, axis=0)\n",
    "        agg_array_name = f\"agg_array_{year}{month}{day_str}.npy\"\n",
    "        np.save(os.path.join(agg_folder_path, agg_array_name), aggregated_array)\n",
    "\n",
    "        # print(f\"Shape of the aggregated file of the date {day_str}/{month}/{year} is:\", aggregated_array.shape)\n",
    "\n",
    "        # Remove the original folder containing ASCII files\n",
    "        shutil.rmtree(sub_folder_path)\n",
    "\n",
    "    print(\"Step 5: The Numpy arrays derived from the aggregated files are complete!\")\n",
    "    \n",
    "#aggregate_array(directory_path, year, month)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "864048b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "\n",
    "def compress_aggregated_folder(directory_path, year, month, output_compressed_path):\n",
    "    print(f\"Entered compress_aggregated_folder with {directory_path}, {year}, {month}, {output_compressed_path}\")\n",
    "    \n",
    "    folder_name_comp = f'YW2017.002_{year}{month}_asc'\n",
    "    folder_path_comp = os.path.join(directory_path, folder_name_comp)\n",
    "    \n",
    "    print(f\"Constructed folder path: {folder_path_comp}\")\n",
    "    \n",
    "    if not os.path.exists(folder_path_comp):\n",
    "        print(f\"Folder '{folder_path_comp}' doesn't exist. Skipping.\")\n",
    "        return\n",
    "\n",
    "    zip_name = f\"{folder_name_comp}.zip\"\n",
    "    zip_path = os.path.join(output_compressed_path, zip_name)\n",
    "    \n",
    "    print(f\"Starting compression from {folder_path_comp} to {zip_path}\")\n",
    "\n",
    "    with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
    "        for root, dirs, files in os.walk(folder_path_comp):\n",
    "            for file in files:\n",
    "                zipf.write(os.path.join(root, file), os.path.relpath(os.path.join(root, file), folder_path_comp))\n",
    "        \n",
    "    print(\"Step 6: Compression complete.\")\n",
    "\n",
    "#compress_aggregated_folder(directory_path, year, month, output_compressed_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8596d884",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_new_array(directory_path, year, month, coordinates):\n",
    "    # Extract the coordinates\n",
    "    min_x = min(coord[0] for coord in coordinates)\n",
    "    max_x = max(coord[0] for coord in coordinates)\n",
    "    min_y = min(coord[1] for coord in coordinates)\n",
    "    max_y = max(coord[1] for coord in coordinates)\n",
    "\n",
    "    if min_x >= max_x or min_y >= max_y:\n",
    "        print(\"Error: Minimum value must be less than maximum value for x and y.\")\n",
    "        return\n",
    "\n",
    "    for day in range(1, 32):\n",
    "        day_str = str(day).zfill(2)\n",
    "        npy_file_name = f\"agg_array_{year}{month}{day_str}.npy\"\n",
    "        npy_file_path = Path(directory_path) / f\"YW2017.002_{year}{month}_asc\" / f\"agg_YW2017.002_{year}{month}{day_str}_asc\" / npy_file_name\n",
    "\n",
    "        if not npy_file_path.exists():\n",
    "            print(f\"No .npy file found for the date {day_str}/{month}/{year}\")\n",
    "            continue\n",
    "\n",
    "        # Load the original array from .npy file\n",
    "        original_array = np.load(npy_file_path)\n",
    "\n",
    "        # Create a window of the desired region\n",
    "        window = original_array[:, min_y:max_y, min_x:max_x]\n",
    "\n",
    "        # Save the new array to a new .npy file\n",
    "        new_file_path = npy_file_path.parent.parent.parent / f\"Window_array_{year}{month}\" / f\"window_{year}{month}{day_str}.npy\"\n",
    "        new_file_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        np.save(new_file_path, window)\n",
    "\n",
    "        print(f\"Shape of the sliced array for the date {day_str}/{month}/{year} is:\", window.shape)\n",
    "\n",
    "    # After processing all days of the month:\n",
    "    original_folder_path = Path(directory_path) / f\"YW2017.002_{year}{month}_asc\"\n",
    "    if original_folder_path.exists():\n",
    "        shutil.rmtree(original_folder_path)\n",
    "        print(f\"Original folder {original_folder_path} has been removed.\")\n",
    "\n",
    "    print(\"Step 7: New arrays have been generated for all available files in the directory!\")\n",
    "#generate_new_array(directory_path, year, month, coordinates)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "869cdd53",
   "metadata": {},
   "source": [
    "### Execute the pipeline for processing image data if it is needed for over a year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21ce7918",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline(directory_path, start_year, end_year, coordinates, output_compressed_path):\n",
    "    for year in range(start_year, end_year + 1):\n",
    "        for month in range(1, 13):  # Loop through months 1 to 12\n",
    "            month_str = str(month).zfill(2)\n",
    "            year_str = str(year)\n",
    "            \n",
    "            print(f\"Processing data for {year_str}-{month_str}...\")\n",
    "\n",
    "            # Download the weather data.\n",
    "            download_weather_data(directory_path, year_str, month_str)\n",
    "\n",
    "            # Extract the data from the tar archive.\n",
    "            extract_tar(directory_path)\n",
    "\n",
    "            # Unzip the gz files.\n",
    "            Unzip_gz_files(directory_path)\n",
    "\n",
    "            # Extract the zip files.\n",
    "            extract_zip(directory_path)\n",
    "            \n",
    "            # Aggregate the data into a single array.\n",
    "            aggregate_array(directory_path, year_str, month_str)\n",
    "            \n",
    "            compress_aggregated_folder(directory_path, year_str, month_str, output_compressed_path)\n",
    "            \n",
    "            # Generate new arrays for all coordinates.\n",
    "            generate_new_array(directory_path, year_str, month_str, coordinates)\n",
    "                        \n",
    "\n",
    "    print(\"All steps have been completed successfully!\")\n",
    "\n",
    "# Define start and end years\n",
    "start_year = int(input(\"Enter the start year: \"))\n",
    "end_year = int(input(\"Enter the end year: \"))\n",
    "\n",
    "# Define coordinates\n",
    "coordinates = [(466, 445), (466, 452), (475, 445), (475, 452)]\n",
    "\n",
    "# Define the directory path\n",
    "\n",
    "directory_path = \"/home/arman_abouali/Downloads/DWD\"  \n",
    "output_compressed_path = \"/home/arman_abouali/Downloads/DWD/Original_files\"\n",
    "# Call the pipeline function\n",
    "pipeline(directory_path, start_year, end_year, coordinates, output_compressed_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6778c48f",
   "metadata": {},
   "source": [
    "### For slicsing original files according to new coordinates, utilize the following two cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8241516",
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "def generate_new_array(directory_path, temp_unzip_folder, output_path, year, month, coordinates):\n",
    "    # Extract the coordinates\n",
    "    min_x = min(coord[0] for coord in coordinates)\n",
    "    max_x = max(coord[0] for coord in coordinates)\n",
    "    min_y = min(coord[1] for coord in coordinates)\n",
    "    max_y = max(coord[1] for coord in coordinates)\n",
    "\n",
    "    if min_x >= max_x or min_y >= max_y:\n",
    "        print(\"Error: Minimum value must be less than maximum value for x and y.\")\n",
    "        return\n",
    "\n",
    "    for day in range(1, 32):\n",
    "        day_str = str(day).zfill(2)\n",
    "        npy_file_name = f\"agg_array_{year}{month}{day_str}.npy\"\n",
    "        npy_file_path = Path(directory_path) / temp_unzip_folder/ f\"agg_YW2017.002_{year}{month}{day_str}_asc\" / npy_file_name\n",
    "\n",
    "        if not npy_file_path.exists():\n",
    "            print(f\"No .npy file found for the date {day_str}/{month}/{year}\")\n",
    "            continue\n",
    "\n",
    "        # Load the original array from .npy file\n",
    "        original_array = np.load(npy_file_path)\n",
    "\n",
    "        # Create a window of the desired region\n",
    "        window = original_array[:, min_y:max_y, min_x:max_x]\n",
    "\n",
    "        # Save the new array to a new .npy file\n",
    "        new_file_path = Path(output_path) / f\"Window_array_{year}{month}\" / f\"window_{year}{month}{day_str}.npy\"\n",
    "        new_file_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        np.save(new_file_path, window)\n",
    "\n",
    "        print(f\"Shape of the sliced array for the date {day_str}/{month}/{year} is:\", window.shape)\n",
    "\n",
    "    # After processing all days of the month:\n",
    "    original_folder_path = Path(directory_path) / f\"YW2017.002_{year}{month}_asc\"\n",
    "    if original_folder_path.exists():\n",
    "        shutil.rmtree(original_folder_path)\n",
    "        print(f\"Original folder {original_folder_path} has been removed.\")\n",
    "\n",
    "    print(\"Step 6: New arrays have been generated for all available files in the directory!\")\n",
    "\n",
    "\n",
    "def process_single_zip(directory_path, output_path, year, month, coordinates):\n",
    "    # Step 1: Target a specific .zip folder based on the given year and month\n",
    "    zip_file_name = f\"YW2017.002_{year}{month}_asc.zip\"\n",
    "    original_zip_path = os.path.join(directory_path, zip_file_name)\n",
    "    \n",
    "    if not os.path.exists(original_zip_path):\n",
    "        print(f\"{zip_file_name} does not exist in {directory_path}\")\n",
    "        return\n",
    "    \n",
    "    # Step 2: Create temporary folder and unzip all contents of the zip file there\n",
    "    temp_unzip_folder = os.path.join(directory_path, 'temp_unzip_folder')\n",
    "    with zipfile.ZipFile(original_zip_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(temp_unzip_folder)\n",
    "\n",
    "    # Step 3: Execute the generate_new_array function based on its new location\n",
    "    generate_new_array(directory_path, temp_unzip_folder, output_path, year, month, coordinates)\n",
    "\n",
    "    # Step 4: Remove the temporary unzipped folder\n",
    "    temp_unzip_folder = os.path.join(directory_path, 'temp_unzip_folder')\n",
    "    shutil.rmtree(temp_unzip_folder)\n",
    "    print(\"Temporary folder removed, and no redundant unzipping in output_path.\")\n",
    "\n",
    "#directory_path = \"/home/arman_abouali/Downloads/DWD/Original_files\"\n",
    "#output_path = '/home/arman_abouali/Downloads/DWD/Original_files/DWD_1by1_old'\n",
    "#coordinates = [(470, 444), (470, 445), (471, 444), (471, 445)]\n",
    "#year = \"2003\"\n",
    "#month = \"11\"\n",
    "\n",
    "#process_single_zip(directory_path, output_path, year, month, coordinates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2212f42c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_all_zips(directory_path, output_path, coordinates, start_year, end_year):\n",
    "    for year in range(start_year, end_year + 1):\n",
    "        for month in range(1, 13):  # Loop through months 1 to 12\n",
    "            month_str = str(month).zfill(2)\n",
    "            year_str = str(year)\n",
    "            \n",
    "            # Process this single zip file\n",
    "            process_single_zip(directory_path, output_path, year_str, month_str, coordinates)\n",
    "\n",
    "# Example usage\n",
    "start_year = int(input(\"Enter the start year: \"))\n",
    "end_year = int(input(\"Enter the end year: \"))\n",
    "directory_path = \"/home/arman_abouali/Downloads/DWD/Original_files\"\n",
    "output_path = '/home/arman_abouali/Downloads/DWD/Original_files/DWD_1by1_old'\n",
    "coordinates = [(470, 444), (470, 445), (471, 444), (471, 445)]\n",
    "#coordinates = [(467, 444), (467, 452), (475, 444), (475, 452)]\n",
    "process_all_zips(directory_path, output_path, coordinates, start_year, end_year)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdb82aac",
   "metadata": {},
   "source": [
    "### Store the processed files into a Parquet file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42f7e7c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_parquet_file(directory_path, output_file):\n",
    "    dictionary = {}\n",
    "    \n",
    "    for root, dirs, files in os.walk(directory_path):\n",
    "        for file_name in files:\n",
    "            if file_name.endswith('.npy'):\n",
    "                arr = np.load(os.path.join(root, file_name))\n",
    "\n",
    "                try:\n",
    "                    date = file_name.split('_')[1][:-4]\n",
    "                    year, month, day = date[:4], date[4:6].lstrip('0'), date[6:].lstrip('0')\n",
    "                    current_time = datetime.strptime(f'{year}-{month}-{day}T00:00:00', '%Y-%m-%dT%H:%M:%S')\n",
    "                except IndexError:\n",
    "                    print(f\"Error: Could not extract date from file name: {file_name}\")\n",
    "                    continue\n",
    "\n",
    "                for i in range(0, arr.shape[0], 96):\n",
    "                    channel_key = f\"Channel {i//96 + 1}\"\n",
    "                    channel_dict = {}\n",
    "\n",
    "                    for j in range(i, min(i+96, arr.shape[0])):\n",
    "                        if j < arr.shape[0]:\n",
    "                            channel_dict[current_time.strftime(\"%Y%m%d_%H%M\")] = arr[j, :, :].tolist()\n",
    "                        current_time += timedelta(minutes=15)\n",
    "\n",
    "                    dictionary.setdefault(channel_key, {}).update(channel_dict)\n",
    "\n",
    "    parquet_df = pd.DataFrame.from_dict(dictionary, orient='index')\n",
    "    parquet_df = parquet_df.stack().reset_index().rename(columns={'level_0': 'Key', 'level_1': 'Timestamp', 0: 'Value'})\n",
    "    parquet_df = parquet_df.reset_index(drop=True)\n",
    "    parquet_df = parquet_df.drop(columns=['Key'])\n",
    "    parquet_df = parquet_df.rename(columns={'Timestamp': 'Key'})\n",
    "\n",
    "    table = pa.Table.from_pandas(parquet_df)\n",
    "    pq.write_table(table, output_file)\n",
    "    \n",
    "    return parquet_df\n",
    "\n",
    "output_file = \"/home/arman_abouali/Downloads/DWD/X_data.parquet\"\n",
    "print(f\"Parquet file '{output_file}' created successfully.\")\n",
    "parquet_df = create_parquet_file(directory_path, output_file)\n",
    "print(parquet_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edb58d7d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
